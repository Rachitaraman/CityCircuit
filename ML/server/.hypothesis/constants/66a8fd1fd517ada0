# file: C:\Users\91843\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\tensorflow\lite\python\lite.py
# hypothesis_version: 6.148.8

[1000, ',', '.tfrecord', '3', 'DEFAULT', 'None', 'OPTIMIZE_FOR_LATENCY', 'OPTIMIZE_FOR_SIZE', '_implements', 'accumulation_type', 'activations_type', 'allow_bfloat16', 'allow_custom_ops', 'api_implements', 'api_version', 'big', 'buffer_location', 'constfold', 'cpu', 'debug_info', 'default_ranges_stats', 'dump_graphviz_dir', 'dump_graphviz_video', 'enable_timing', 'function', 'graph_debug_info', 'hlo', 'inference_input_type', 'inference_type', 'input_content', 'input_format', 'input_names', 'ir_dump_dir', 'ir_dump_func_regex', 'ir_dump_pass_regex', 'is_proto_format', 'layout', 'lite.Optimize', 'lite.TFLiteConverter', 'lite.TargetSpec', 'lite.TocoConverter', 'little', 'optimization_default', 'optimization_qat', 'output_format', 'preserve_assert_op', 'print_ir_after', 'print_ir_before', 'qdq_conversion_mode', 'quantization_config', 'quantization_options', 'quantize_to_float16', 'rb', 'saved_model_dir', 'saved_model_tags', 'saved_model_version', 'select_user_tf_ops', 'serve', 'serving_default', 'strict_qdq_mode', 'supported_backends', 'target_ops', 'tf', 'tf.', 'tf_quantization_mode', 'tf_version', 'unfold_batchmatmul', 'use_buffer_offset', 'utf-8']